{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import clean as c\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import rolling_mean as rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2004 = pd.read_csv('data/2004.csv')\n",
    "s2005 = pd.read_csv('data/2005.csv')\n",
    "s2006 = pd.read_csv('data/2006.csv')\n",
    "s2007 = pd.read_csv('data/2007.csv')\n",
    "s2008 = pd.read_csv('data/2008.csv')\n",
    "s2009 = pd.read_csv('data/2009.csv')\n",
    "s2010 = pd.read_csv('data/2010.csv')\n",
    "s2011 = pd.read_csv('data/2011.csv')\n",
    "s2012 = pd.read_csv('data/2012.csv')\n",
    "s2013 = pd.read_csv('data/2013.csv')\n",
    "s2014 = pd.read_csv('data/2014.csv')\n",
    "s2015 = pd.read_csv('data/2015.csv')\n",
    "s2016 = pd.read_csv('data/2016.csv')\n",
    "s2017 = pd.read_csv('data/2017.csv')\n",
    "s2018 = pd.read_csv('data/2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean all season.\n",
    "There are two different ways of cleaning that are needed because the New Orleans used to be the \"Hornets\" but now\n",
    "Charlotte is the \"Hornets\". When changing team names to be consistent through all seasons, the order of cleaning matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2004 = c.all_clean_pels(s2004)\n",
    "s2005 = c.all_clean_pels(s2005)\n",
    "s2006 = c.all_clean_pels(s2006)\n",
    "s2007 = c.all_clean_pels(s2007)\n",
    "s2008 = c.all_clean_pels(s2008)\n",
    "s2009 = c.all_clean_pels(s2009)\n",
    "s2010 = c.all_clean_pels(s2010)\n",
    "s2011 = c.all_clean_pels(s2011)\n",
    "s2012 = c.all_clean_pels(s2012)\n",
    "s2013 = c.all_clean_pels(s2013)\n",
    "s2014 = c.all_clean(s2014)\n",
    "s2015 = c.all_clean(s2015)\n",
    "s2016 = c.all_clean(s2016)\n",
    "s2017 = c.all_clean(s2017)\n",
    "s2018 = c.all_clean(s2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing how to set up the rolling means\n",
    "With rolling means there are a lot of rows that are filled with zeros. These need to be taken out of the feature matrix in order to properly train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll2018 = rm.complete_rolling_means(s2018,games_back=7,games_needed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roll2018[roll2018['Min']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the feature matrix for modelling\n",
    "As I have it set up to this point, the feature matrix has two rows per game, one for each team. This needs to be combined into a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = roll2018.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2 = []\n",
    "for i in range(int(len(mat)/2)):\n",
    "    mat2.append(list(mat[2*i])+(list(mat[2*i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ('team1_'+roll2018.columns).append('team2_'+roll2018.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the indices\n",
    "In order to set the indices for the new feature matrix, I need to grab the game ids from the old one. This would be grabbing each game id twice, so grabbing only the even row game id indices is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll2018.index.get_level_values('game_id')[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mat2,columns=cols,index=roll2018.index.get_level_values('game_id')[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['team2_Total_PTS','team1_Min','team2_Min','team1_home_team','team2_home_team','team1_starter','team2_starter'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final steps of setting up the feature matrix\n",
    "I removed the first 100 rows of the feature matrix to ensure that no row has a team where the rolling mean is not calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_use.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Rolling Mean Model\n",
    "This model uses a train test split on a season to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df_use.drop('team1_Total_PTS',axis=1),df_use['team1_Total_PTS'],test_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha':[.93,.95,.97,.96,.94], 'normalize':[True, False],'tol':[.001,.002,.003,.00008]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(las, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = Lasso(alpha=.97,tol=.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = las.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My predictions vs the actual total scores\n",
    "A dataframe that contains the real score, my predictions, and the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame(y_test.values,columns=['real'],index = y_test.index.get_level_values('game_id'))\n",
    "comp_df['predictions']=preds\n",
    "comp_df['differences']=comp_df['real']-comp_df['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average difference between my prediction and the real score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(y_test-preds))/len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Rolling Means Model\n",
    "This model will combine seasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rolling_model as rmod\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All seasons\n",
    "Train on seasons 2004-2011. train=True will end up removing all game that went to overtime as this adds more time to games. These games should not be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018=rmod.get_train_test(s2018,6,5)\n",
    "df2017=rmod.get_train_test(s2017,6,5)\n",
    "df2016=rmod.get_train_test(s2016,6,5)\n",
    "df2015=rmod.get_train_test(s2015,6,5)\n",
    "df2014=rmod.get_train_test(s2014,6,5)\n",
    "#df2013,(X_train2013,X_test2013,y_train2013,y_test2013)=rmod.get_train_test(s2013)\n",
    "df2012=rmod.get_train_test(s2012,6,5)\n",
    "df2011=rmod.get_train_test(s2011,6,5,train=True)\n",
    "df2010=rmod.get_train_test(s2010,6,5,train=True)\n",
    "df2009=rmod.get_train_test(s2009,6,5,train=True)\n",
    "df2008=rmod.get_train_test(s2008,6,5,train=True)\n",
    "df2007=rmod.get_train_test(s2007,6,5,train=True)\n",
    "df2006=rmod.get_train_test(s2006,6,5,train=True)\n",
    "df2005=rmod.get_train_test(s2005,6,5,train=True)\n",
    "df2004=rmod.get_train_test(s2004,6,5,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test sets\n",
    "These dataframes need to be combined. Then the target needs to be taken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((df2011,df2010,df2009,df2008,df2007,df2006,df2005,df2004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.concat((df2018,df2017,df2016,df2015,df2014,df2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.drop('team1_Total_PTS',axis=1)\n",
    "X_test=test.drop('team1_Total_PTS',axis=1)\n",
    "y_train=train['team1_Total_PTS']\n",
    "y_test=test['team1_Total_PTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best parameters for the Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmod.find_params(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the lasso model, the R^2 value average difference, and the comparison dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las, r2,diff, df2 = rmod.model(X_train,X_test,y_train,y_test,1.5,.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look to see which features the lasso captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(train.columns)\n",
    "names.remove('team1_Total_PTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[las.coef_!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of real score vs predicted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(df2['real'],df2['predictions'])\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Real Score vs Predicted Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[40,50,60,70],'min_samples_leaf':[12,14,16]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(rf,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=60,min_samples_leaf=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both the R^2 and differences were worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(y_test-pred))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = rid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of the box score and difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(y_test-pred))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = parameters = {'alpha':[1000,10000,3000,5000,7000],'tol':[.001,.002,.003,.0009,.0008,.0007]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(rid,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something wrong with the grid search\n",
    "The grid search is returning best alpha as 1000, however the model can be improved by using a much higher alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = Ridge(alpha=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = rid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(y_test-pred))/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proof a higher alpha is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = Ridge(alpha=45000)\n",
    "rid = rid.fit(X_train,y_train)\n",
    "print(rid.score(X_test,y_test))\n",
    "pred = rid.predict(X_test)\n",
    "sum(np.abs(y_test-pred))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using only 2018 as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018=rmod.get_train_test(s2018,6,5)\n",
    "df2017=rmod.get_train_test(s2017,6,5,train=True)\n",
    "df2016=rmod.get_train_test(s2016,6,5,train=True)\n",
    "df2015=rmod.get_train_test(s2015,6,5,train=True)\n",
    "df2014=rmod.get_train_test(s2014,6,5,train=True)\n",
    "#df2013,(X_train2013,X_test2013,y_train2013,y_test2013)=rmod.get_train_test(s2013)\n",
    "df2012=rmod.get_train_test(s2012,6,5,train=True)\n",
    "df2011=rmod.get_train_test(s2011,6,5,train=True)\n",
    "df2010=rmod.get_train_test(s2010,6,5,train=True)\n",
    "df2009=rmod.get_train_test(s2009,6,5,train=True)\n",
    "df2008=rmod.get_train_test(s2008,6,5,train=True)\n",
    "df2007=rmod.get_train_test(s2007,6,5,train=True)\n",
    "df2006=rmod.get_train_test(s2006,6,5,train=True)\n",
    "df2005=rmod.get_train_test(s2005,6,5,train=True)\n",
    "df2004=rmod.get_train_test(s2004,6,5,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((df2017,df2016,df2015,df2014,df2012,df2011))#,df2010,df2009,df2008,df2007,df2006,df2005,df2004))\n",
    "test=df2018\n",
    "X_train=train.drop('team1_Total_PTS',axis=1)\n",
    "X_test=test.drop('team1_Total_PTS',axis=1)\n",
    "y_train=train['team1_Total_PTS']\n",
    "y_test=test['team1_Total_PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmod.find_params(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las, r2,diff, df2 = rmod.model(X_train,X_test,y_train,y_test,.9,.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "Check to see how the linear regression model using only the performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linreg.fit(X_train[['team1_+/-', 'team1_PTS', 'team1_REB', 'team1_TO',\n",
    "       'team1_3PT_attempts', 'team1_FG_attempts', 'team1_DEF_PTS',\n",
    "       'team2_+/-', 'team2_AST', 'team2_PTS', 'team2_REB', 'team2_TO',\n",
    "       'team2_FT_attempts', 'team2_3PT_attempts', 'team2_FG_attempts',\n",
    "       'team2_DEF_PTS']],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linreg.predict(X_test[['team1_+/-', 'team1_PTS', 'team1_REB', 'team1_TO',\n",
    "       'team1_3PT_attempts', 'team1_FG_attempts', 'team1_DEF_PTS',\n",
    "       'team2_+/-', 'team2_AST', 'team2_PTS', 'team2_REB', 'team2_TO',\n",
    "       'team2_FT_attempts', 'team2_3PT_attempts', 'team2_FG_attempts',\n",
    "       'team2_DEF_PTS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X_test[['team1_+/-', 'team1_PTS', 'team1_REB', 'team1_TO',\n",
    "       'team1_3PT_attempts', 'team1_FG_attempts', 'team1_DEF_PTS',\n",
    "       'team2_+/-', 'team2_AST', 'team2_PTS', 'team2_REB', 'team2_TO',\n",
    "       'team2_FT_attempts', 'team2_3PT_attempts', 'team2_FG_attempts',\n",
    "       'team2_DEF_PTS']],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(y_test-preds))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
